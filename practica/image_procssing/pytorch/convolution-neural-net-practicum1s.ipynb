{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2ad83ef-4ee3-4463-9b5e-127f12237363",
   "metadata": {},
   "source": [
    "<a href=\"https://it-omscholing.nl/locaties/hogeschool-rotterdam/\">\n",
    "<div>\n",
    "<a><img src='../../pics/banner.PNG'/></a>\n",
    "</div>\n",
    "<div>\n",
    "<a href=''><img src='../../pics/miw.PNG'/></a>\n",
    "</div>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf243b00-2d08-48b7-85cf-b9f2c288d81d",
   "metadata": {},
   "source": [
    "# Practicum Convolution Neural Nets (CNN) Deel 1\n",
    "\n",
    "**Doel: Toepassen Convolutional Neural Networks**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f0beb08-683b-4d1a-8a3b-7c2938aef74f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2ed2f-2d11-4d51-a10f-02ff61588ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision.io import read_image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef8f9f9b-8024-493f-aa94-b6eb3ad10184",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Globale variabelen</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae222e-fc69-47b6-bb65-ad38f29c38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "forestDirectory = '../../pics/2750/River'\n",
    "industrialDirectory = '../../pics/2750/Industrial'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab893eb4-4667-409f-9f7d-1f2e708be670",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "<p>\n",
    "Dit practicum bestaat uit twee onderdelen\n",
    "<ol>\n",
    "    <li><b>Het toepassen van een convolutie en pooling filter</b></li>\n",
    "    <li>Het bouwen en trainen van een eenvoudig convolutional neural net dat een industrieterrein van een bos kan onderscheiden. Voor beide oefeningen gebruiken we de EuroSAT_RGB dataset</li>\n",
    "</ol>    \n",
    "</p>\n",
    "\n",
    "<img src='../../pics/eurosat_cnn.png' length=65% width=65%>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5305365c-a61c-4dea-adf8-2e9dddef9b82",
   "metadata": {},
   "source": [
    "<h3>Data Collection</h3>\n",
    "<p>\n",
    "We gebruiken Images uit de <a href=\"https://github.com/phelber/EuroSAT\">EuroSat dataset</a> die gemaakt zijn met de Sentinel-2 sateliet. Elke image is een 64x64 pixels foto van Europees aardoppervlak op een hoogte van 10 meter. De images zijn te categoriseren in Highway, Industrial, Pasture, PermanentCrop, Residential, River en SeaLake.\n",
    "</p>\n",
    "<img src=../../pics/eurosat_overview_small.jpg length=40% width=40%>\n",
    "<p>\n",
    "Download <a href=\"http://madm.dfki.de/files/sentinel/EuroSAT.zip\">EuroSAT.zip</a> en kopieer daaruit de directory 2750 naar opdrachten/practica/pics.      \n",
    "<strong>Voeg het pad naar de directory 2750 toe aan .gitignore zodat je de plaatjes niet naar je remote git repository pusht</strong>\n",
    "</p>    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5da250e7-2075-411c-b097-6bf232981619",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Opdrachten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41b5b973-72e7-4159-88eb-ea86258a26fd",
   "metadata": {},
   "source": [
    "### Opdracht 1: Afbeelding inladen\n",
    "\n",
    "PyTorch module <u>read_image</u> maakt het inladen van afbeeldingen als Tensor gemakkelijk. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ea14f80-0831-4620-8c4c-280047047bc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p>\n",
    "Bekijk de documentatie van de <a href=\"https://pytorch.org/vision/stable/io.html\">PyTorch io module</a> en laad de afbeelding 'Industrial_1.jpg' in variable <u>industrial</u> als een 1-dimensionale Torch Tensor met float (Scalar) waarden.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3504f-8d2c-488f-bc10-aa94d70a0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oplossing\n",
    "import torch\n",
    "from torchvision import io\n",
    "\n",
    "image_path = '../../pics/2750/Industrial'\n",
    "industrial = io.read_image('../../pics/2750/Industrial/Industrial_1.jpg')\n",
    "industrial = industrial.flatten()\n",
    "industrial = industrial.float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9380530f-1361-4def-986a-15fe316cc63f",
   "metadata": {},
   "source": [
    "### Opdracht 2: convolutie + padding uitvoeren\n",
    "\n",
    "<p>\n",
    "De basis-ingrediÃ«nten van een CNN, convolutie en pooling, hebben we al met de hand uitgevoerd tijdens het practicum Numpy 2.\n",
    "Ditmaal voeren we een convolutie filter uit met <a href=\"https://pytorch.org/docs/stable/nn.functional.html\">PyTorch Functional</a>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "765ee9d7-b86b-4e33-8783-981a57019bc6",
   "metadata": {},
   "source": [
    "<p>\n",
    "Maak een <a href=\"https://en.wikipedia.org/wiki/Kernel_(image_processing)\">3 x 3 kernel</a> om een edge te detecteren en representeer de <u>square kernel met equal stride</u> als een Pytorch Tensor met naam <u>edgeFilter</u> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e55e7-c6a7-4a9b-a38f-83db18d5d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the kernel values for edge detection\n",
    "kernel_values = torch.tensor([\n",
    "    [-1, -1, -1],\n",
    "    [-1, 8, -1],\n",
    "    [-1, -1, -1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Reshape the kernel to match the expected dimensions\n",
    "edgeFilter = kernel_values.view(1, 1, 3, 3)\n",
    "\n",
    "print(edgeFilter)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd1d2a17-1c9c-472c-a55e-fba7d1ba2977",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Maak o.b.v. <u>edgeFiler</u> een Conv2d filter met als\n",
    "    <ul>\n",
    "        <li>naam <u>edgeConv</u></li>\n",
    "        <li>kernel_size=3</li>\n",
    "        <li>stride=1 (default) geeft aan hoeveel de kernel verplaatst per stap</li>\n",
    "        <li>padding=0 (default) geeft aan hoe we omgaan met de randen</li>\n",
    "        <li>requires_grad=True</li>\n",
    "    </ul>\n",
    "</p>            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569e457-5497-4282-86c8-1d634bf4f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oplossing\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create the Conv2d filter\n",
    "edgeConv = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "# Set the weights of the filter to edgeFilter\n",
    "with torch.no_grad():\n",
    "    edgeConv.weight.copy_(edgeFilter)\n",
    "\n",
    "# Set requires_grad=True to enable gradients for the filter\n",
    "edgeConv.weight.requires_grad_(True)\n",
    "\n",
    "print(edgeConv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dacc3ba6-dfa6-4be6-b877-30d379e41ae9",
   "metadata": {},
   "source": [
    "<p>Pas je ontworpen filter toe op 'Industrial_1.jpg'</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c62035-ea2b-4f44-a2cc-7d48c757b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision import transforms\n",
    "#from PIL import Image\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '../../pics/2750/Industrial/Industrial_1.jpg'\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(),\n",
    "])\n",
    "image = preprocess(Image.open(image_path))\n",
    "\n",
    "# Apply the edgeFilter to the image\n",
    "output = nn.functional.conv2d(image.unsqueeze(0), edgeFilter, stride=1, padding=1)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02746de6-89ad-4ed1-ae9c-457e528fcf03",
   "metadata": {},
   "source": [
    "<p>Gebruik <u>plt.subplots</u> voor het naast elkaar weergeven van Images <u>industrial</u> en <u>industrialFiltered</u></p>\n",
    "<note>HINT: kijk in notebook Numpy Opdracht 1 uit periode 1</note>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21634fcd-a9e7-428a-9758-ab6801b3c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from PIL import Image\n",
    "#import numpy as np\n",
    "\n",
    "# Load the original image using PIL.Image\n",
    "image_path = '../../pics/2750/Industrial/Industrial_1.jpg'\n",
    "original_image = Image.open(image_path)\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "original_image_array = np.array(original_image)\n",
    "\n",
    "# Flatten the image array\n",
    "flattened_image = original_image_array.flatten()\n",
    "\n",
    "# Reshape the flattened image back to the original shape\n",
    "reshaped_image = flattened_image.reshape(original_image_array.shape)\n",
    "\n",
    "# Define the subplot layout\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "# Plot the original image\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Plot the filtered image\n",
    "axes[1].imshow(output.squeeze().detach().numpy(), cmap='Set1')\n",
    "axes[1].set_title('Filtered Image')\n",
    "\n",
    "# Remove axis ticks\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust the subplot layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92d75d51-0377-4f37-a9e9-92ec772e85e1",
   "metadata": {},
   "source": [
    "**Herhaal bovenstaande stappen voor een Forest Image uit de EuroSAT Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228aa7d-b999-405e-b762-aaa9773c87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path1 = '../../pics/2750/Forest'\n",
    "forest = io.read_image('../../pics/2750/Forest/Forest_302.jpg')\n",
    "foresgt = forest.flatten()\n",
    "forest = forest.float()\n",
    "\n",
    "image_path1 = '../../pics/2750/Forest/Forest_302.jpg'\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(),\n",
    "])\n",
    "image = preprocess(Image.open(image_path1))\n",
    "\n",
    "# Apply the edgeFilter to the image\n",
    "output = nn.functional.conv2d(image.unsqueeze(0), edgeFilter, stride=1, padding=1)\n",
    "print(output)\n",
    "\n",
    "# Oplossing\n",
    "image_path1 = '../../pics/2750/Forest/Forest_302.jpg'\n",
    "original_image = Image.open(image_path1)\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "original_image_array = np.array(original_image)\n",
    "\n",
    "# Flatten the image array\n",
    "flattened_image = original_image_array.flatten()\n",
    "\n",
    "# Reshape the flattened image back to the original shape\n",
    "reshaped_image = flattened_image.reshape(original_image_array.shape)\n",
    "\n",
    "# Define the subplot layout\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "# Plot the original image\n",
    "axes[0].imshow(original_image)\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "# Plot the filtered image\n",
    "axes[1].imshow(output.squeeze().detach().numpy(), cmap='Paired')\n",
    "axes[1].set_title('Filtered Image')\n",
    "\n",
    "# Remove axis ticks\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust the subplot layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6574b602-78bb-4bd0-b52b-5508f69cbbaf",
   "metadata": {},
   "source": [
    "### Reflectie\n",
    "\n",
    "Zijn 'Industrial_1.jpg' en 'Forrest_1.jpg' na een convolutie beter te onderscheiden?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9423c15a-b8f7-4140-9ecf-a37a8c13fdfa",
   "metadata": {},
   "source": [
    "**Extra: varieer de stride en de padding en toon de resultaten**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83a3dbb4",
   "metadata": {},
   "source": [
    "### Reflectie\n",
    "\n",
    "Het effect van de convolutie hangt af van verschillende factoren, waaronder de gebruikte kernel en het specifieke doel van de convolutie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7796dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '../../pics/2750/Industrial/Industrial_1.jpg'\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(),\n",
    "])\n",
    "image = preprocess(Image.open(image_path))\n",
    "\n",
    "# Define the square kernel\n",
    "kernel_size = 3\n",
    "kernel = torch.ones(kernel_size, kernel_size) / (kernel_size * kernel_size)\n",
    "\n",
    "# Define the stride and padding values to try\n",
    "stride_values = [1, 2, 3]\n",
    "padding_values = [0, 1, 2]\n",
    "\n",
    "# Create a subplot grid to display the results\n",
    "num_rows = len(stride_values)\n",
    "num_cols = len(padding_values)\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "\n",
    "# Loop over the stride and padding values\n",
    "for i, stride in enumerate(stride_values):\n",
    "    for j, padding in enumerate(padding_values):\n",
    "        # Apply the convolution with the current stride and padding\n",
    "        output = F.conv2d(image.unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0), stride=stride, padding=padding)\n",
    "\n",
    "        # Plot the filtered image in the corresponding subplot\n",
    "        axes[i, j].imshow(output.squeeze().detach().numpy(), cmap='Paired')\n",
    "        axes[i, j].set_title(f'Stride={stride}, Padding={padding}')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "# Adjust the subplot layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b682a804-e1f7-4b9d-a3fe-2de933c32f44",
   "metadata": {},
   "source": [
    "### Bronnen\n",
    "\n",
    "[EuroSAT project](https://github.com/phelber/eurosat)\n",
    "\n",
    "[Pytorch Neural Nets](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "[Kernels](https://en.wikipedia.org/wiki/Kernel_(image_processing))\n",
    "\n",
    "[A simple CNN with Pytorch](https://tomroth.com.au/pytorch-cnn)\n",
    "\n",
    "[A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\n",
    "\n",
    "[Using Dropout Regularization in PyTorch Models](https://machinelearningmastery.com/using-dropout-regularization-in-pytorch-models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
